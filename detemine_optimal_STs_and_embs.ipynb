{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b2590b",
   "metadata": {},
   "source": [
    "THE SCRIPT CANT BE RUN ON ITS OWN. \n",
    "\n",
    "This script assumes that embeddings and clustring has been performed already, and relies on resulting files! \n",
    "\n",
    "This script saves the files which is used by the analysis jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a56936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825ad4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2-C-methyl-D-erythritol_2,4-cyclodiphosphate_synthase': 0.8997, '3-methyl-2-oxobutanoate_hydroxymethyltransferase': 0.9646, '4-hydroxy-3-methylbut-2-en-1-yl_diphosphate_synthase': 0.9381, '4-hydroxy-3-methylbut-2-enyl_diphosphate_reductase': 0.9174, 'ATP_cob(I)alamin_adenosyltransferase': 0.9351, 'ATP--corrinoid_adenosyltransferase': 0.9469, 'RNA_methyltransferase': 0.9292, 'Translation_initiation_factor': 0.9499, 'tRNA_(5-methylaminomethyl-2-thiouridylate)-methyltransferase': 0.9351, 'ligase': 0.9528, 'methyltransferase': 0.9558, '4-diphosphocytidyl-2-C-methyl-D-erythritol_kinase': 0.9351, 'DNA-directed_RNA_polymerase_subunit_beta': 0.9823, 'Ribosomal_RNA_small_subunit_methyltransferase_I': 0.9499, '2-oxoglutarate_dehydrogenase_E1': 0.9528, 'biotin_synthase': 0.9351, 'succinyltransferase': 0.9499, '2-amino-4-hydroxy-6-hydroxymethyldihydropteridine': 0.9233, 'acetolactate_synthase_small_subunit': 0.9617, 'phosphoglycerate_mutase_2_3-bisphosphoglycerate-independent': 0.9558, '2-succinyl-5-enolpyruvyl-6-hydroxy-3-cyclohexene-1-carboxylic-acid_synthase': 0.9145, 'Thiazole_synthase': 0.9528, 'Amidophosphoribosyltransferase': 0.9056, 'ketol-acid_reductoisomerase': 0.9322, 'type_I_DNA_topoisomerase': 0.9528, '4-hydroxy-tetrahydrodipicolinate_reductase': 0.9322, 'Lipid-A-disaccharide_synthase': 0.9204, 'Ribonuclease_PH': 0.9735, 'porphobilinogen_synthase': 0.9735, 'Phosphoserine_aminotransferase': 0.9735, 'Tetraacyldisaccharide_4-kinase': 0.9174, 'queuine_tRNA-ribosyltransferase': 0.9528, '1-4-dihydroxy-2-naphthoate_octaprenyltransferase': 0.9469, 'Nicotinate-nucleotide_pyrophosphorylase': 0.9469}\n"
     ]
    }
   ],
   "source": [
    "# this cell finds all the specific STs for homolog groups.\n",
    "\n",
    "# based on where the tipping point of the clusters is => core to shell\n",
    "\n",
    "path = \"/data/nilar/fastas_for_calibration/cluster_faiss_files\"\n",
    "\n",
    "homolog_group2ST = {}\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    print(f\"Error: Directory not found at '{path}'\")\n",
    "else:\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if file.endswith(\"all_embeddings_fuzz_clustering_v2.csv\"):\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "\n",
    "                    df = pl.read_csv(file_path, separator=',', has_header=True)\n",
    "\n",
    "                    for col_name in df.columns:\n",
    "                        if col_name.endswith(\"category\"):\n",
    "                            \n",
    "\n",
    "                            if not df[col_name].eq(\"core\").any():\n",
    "                                \n",
    "                                split_name = col_name.split('_')\n",
    "                                \n",
    "                                if len(split_name) > 1:\n",
    "                                    file_key = os.path.splitext(file)[0]\n",
    "                                    file_key = file_key.replace(\"_all_embeddings_fuzz_clustering_v2\", \"\").replace(\" \", \"_\")\n",
    "                                    homolog_group2ST[file_key] = float(split_name[1])\n",
    "                                    \n",
    "                                    break\n",
    "                                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process file {file}: {e}\")\n",
    "\n",
    "print(homolog_group2ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell saves all embs into a single df\n",
    "\n",
    "\n",
    "data_directory = '/data/nilar/fastas_for_calibration/busco_fastas_indiva/'\n",
    "all_protein_data = []\n",
    "\n",
    "homolog_groups = list(homolog_group2ST.keys())\n",
    "\n",
    "for group_name in homolog_groups:\n",
    "    group_path = os.path.join(data_directory, group_name)\n",
    "    \n",
    "    if os.path.isdir(group_path):\n",
    "        pt_file = next((f for f in os.listdir(group_path) if f.endswith('.pt')), None)\n",
    "        \n",
    "        if pt_file:\n",
    "            file_path = os.path.join(group_path, pt_file)\n",
    "            try:\n",
    "                embeddings_dict = torch.load(file_path)\n",
    "                optimal_st = homolog_group2ST[group_name]\n",
    "                \n",
    "                for protein_id, embedding in embeddings_dict.items():\n",
    "                    all_protein_data.append({\n",
    "                        'protein_id': protein_id,\n",
    "                        'homolog_group': group_name,\n",
    "                        'st': optimal_st,\n",
    "                        'embedding': embedding.numpy() \n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process file {file_path}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(all_protein_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c11aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-Means sampling to select 20 representatives per group...\n",
      "\n",
      "Sampling complete.\n",
      "Created a new dataset with 640 total representative embeddings.\n",
      "(32 groups x 20 representatives/group approx.)\n"
     ]
    }
   ],
   "source": [
    "# this cell performs K-Means clustering to find representative embeddings\n",
    "\n",
    "K_REPRESENTATIVES = 20 \n",
    "\n",
    "representative_embeddings = []\n",
    "print(f\"Starting K-Means sampling to select {K_REPRESENTATIVES} representatives per group...\")\n",
    "\n",
    "for group_name, group_df in df.groupby('homolog_group'):\n",
    "    \n",
    "    embeddings_matrix = np.vstack(group_df['embedding'].values)\n",
    "    \n",
    "    n_samples = embeddings_matrix.shape[0]\n",
    "    if n_samples == 0:\n",
    "        continue\n",
    "    \n",
    "    k = min(K_REPRESENTATIVES, n_samples)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(embeddings_matrix)\n",
    "    \n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    for center_embedding in cluster_centers:\n",
    "        representative_embeddings.append({\n",
    "            'homolog_group': group_name,\n",
    "            'st': group_df['st'].iloc[0], \n",
    "            'embedding': center_embedding \n",
    "        })\n",
    "\n",
    "df_representatives = pd.DataFrame(representative_embeddings)\n",
    "\n",
    "df_representatives_to_save = df_representatives.copy()\n",
    "df_representatives_to_save['embedding'] = df_representatives_to_save['embedding'].apply(\n",
    "    lambda arr: ' '.join(map(str, arr))\n",
    ")\n",
    "df_representatives_to_save.to_csv(\"/data/nilar/pan_genome/gaussian_process_regression_on_STs/bayesian_similarity/kMeans_embeddings.csv\", index=False)\n",
    "\n",
    "num_groups = len(df_representatives['homolog_group'].unique())\n",
    "\n",
    "print(\"\\nSampling complete.\")\n",
    "print(f\"Created a new dataset with {len(df_representatives)} total representative embeddings.\")\n",
    "print(f\"({num_groups} groups x {K_REPRESENTATIVES} representatives/group approx.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed01c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random sampling to select 20 embeddings per group...\n",
      "\n",
      "Sampling complete.\n",
      "Created a new dataset with 640 total sampled embeddings.\n",
      "(32 groups x up to 20 samples/group)\n"
     ]
    }
   ],
   "source": [
    "# this cell performs random sampling to find representative embeddings\n",
    "\n",
    "SAMPLES_PER_GROUP = 20\n",
    "\n",
    "all_sampled_dfs = []\n",
    "print(f\"Starting random sampling to select {SAMPLES_PER_GROUP} embeddings per group...\")\n",
    "\n",
    "for group_name, group_df in df.groupby('homolog_group'):\n",
    "    \n",
    "    n_available = len(group_df)\n",
    "\n",
    "    n_to_sample = min(SAMPLES_PER_GROUP, n_available)\n",
    "    \n",
    "    if n_to_sample > 0:\n",
    "        sampled_data = group_df.sample(n=n_to_sample, random_state=42)\n",
    "        all_sampled_dfs.append(sampled_data)\n",
    "\n",
    "df_representatives = pd.concat(all_sampled_dfs, ignore_index=True)\n",
    "\n",
    "df_representatives_to_save = df_representatives.copy()\n",
    "df_representatives_to_save['embedding'] = df_representatives_to_save['embedding'].apply(\n",
    "    lambda arr: ' '.join(map(str, arr))\n",
    ")\n",
    "df_representatives_to_save.to_csv(\"/data/nilar/pan_genome/gaussian_process_regression_on_STs/bayesian_similarity/kMeans_embeddings.csv\", index=False)\n",
    "\n",
    "num_groups = len(df['homolog_group'].unique())\n",
    "print(\"\\nSampling complete.\")\n",
    "print(f\"Created a new dataset with {len(df_representatives)} total sampled embeddings.\")\n",
    "print(f\"({num_groups} groups x up to {SAMPLES_PER_GROUP} samples/group)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded95144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean embeddings saved correctly to CSV.\n"
     ]
    }
   ],
   "source": [
    "# this cell performs a mean computaion of all the embs in each group\n",
    "\n",
    "mean_embeddings_list = []\n",
    "for group_name, group_df in df.groupby('homolog_group'):\n",
    "    mean_embedding = np.mean(np.vstack(group_df['embedding'].values), axis=0)\n",
    "    mean_embeddings_list.append({\n",
    "        'homolog_group': group_name,\n",
    "        'mean_embedding': mean_embedding,\n",
    "        'st': group_df['st'].iloc[0] \n",
    "    })\n",
    "\n",
    "mean_embeddings_df = pd.DataFrame(mean_embeddings_list)\n",
    "\n",
    "mean_embeddings_df_to_save = mean_embeddings_df.copy()\n",
    "mean_embeddings_df_to_save['mean_embedding'] = mean_embeddings_df_to_save['mean_embedding'].apply(\n",
    "    lambda arr: ' '.join(map(str, arr))\n",
    ")\n",
    "mean_embeddings_df_to_save.to_csv(\"/data/nilar/pan_genome/gaussian_process_regression_on_STs/bayesian_similarity/mean_embeddings.csv\", index=False)\n",
    "print(\"Mean embeddings saved correctly to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
